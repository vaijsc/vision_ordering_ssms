{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "class SoftSort(torch.nn.Module):\n",
    "    def __init__(self, tau=1.0, hard=False, pow=1.0):\n",
    "        super(SoftSort, self).__init__()\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "        self.pow = pow\n",
    "\n",
    "    def forward(self, scores: Tensor):\n",
    "        \"\"\"\n",
    "        scores: elements to be sorted. Typical shape: batch_size x n\n",
    "        \"\"\"\n",
    "        scores = scores.unsqueeze(-1)\n",
    "        sorted = scores.sort(descending=True, dim=1)[0]\n",
    "        pairwise_diff = (scores.transpose(1, 2) - sorted).abs().pow(self.pow).neg() / self.tau\n",
    "        P_hat = pairwise_diff.softmax(-1)\n",
    "\n",
    "        if self.hard:\n",
    "            P = torch.zeros_like(P_hat, device=P_hat.device)\n",
    "            P.scatter_(-1, P_hat.topk(1, -1)[1], value=1)\n",
    "            P_hat = (P - P_hat).detach() + P_hat\n",
    "        return P_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "ss = SoftSort(hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0710,  1.0674, -0.1502]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([[3.0,2.0,5.0]])\n",
    "mat = ss(-value)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0674,  1.0710, -0.1502]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', mat, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 5.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', mat, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 49])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming dot_prod is your input tensor with shape [128, 49]\n",
    "dot_prod = torch.randn(128, 49)  # Example input; replace with your actual tensor\n",
    "\n",
    "# Create an instance of SoftSort\n",
    "soft_sort = SoftSort(tau=1.0, hard=True)\n",
    "\n",
    "# Use SoftSort to rearrange the values based on dot_prod\n",
    "rearranged_values = soft_sort(dot_prod)\n",
    "\n",
    "# The rearranged_values will have the same shape as dot_prod\n",
    "print(rearranged_values.shape)  # Sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1124,  0.3074, -1.1164,  0.3205, -1.1716,  2.3287, -0.8495,  1.1453,\n",
       "         0.5923, -1.5569, -0.1205, -0.8293, -0.6210,  0.5117, -0.3681,  0.2268,\n",
       "         1.3512,  0.9039, -1.2890,  0.0369, -0.4759,  1.0174, -0.2304, -1.6933,\n",
       "        -1.0110,  0.5619, -1.6541, -1.0059, -0.8064, -0.7143,  0.1221,  0.5179,\n",
       "         1.4403, -0.7206,  1.5478,  0.7394,  0.2535,  0.7082, -2.4703,  1.3706,\n",
       "        -0.3273, -0.7106, -1.3301, -2.0515, -0.0367, -1.3309,  0.0812,  1.7324,\n",
       "        -1.0255])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6355,  1.7180, -0.5275, -0.8325, -0.7621,  0.5230,  1.1946,  0.2854,\n",
       "        -0.3085, -0.0471, -2.4254, -0.4359, -0.7054, -1.3043, -0.3422, -0.0800,\n",
       "         1.1700, -1.1339, -1.0904, -1.1093,  0.3365,  0.6108, -2.7919, -1.8271,\n",
       "        -0.0098, -0.3422, -0.5166,  1.1289,  1.1492, -1.1392,  2.1992, -1.3139,\n",
       "         2.1645,  0.2463,  1.5276, -2.4445, -1.2331,  0.8298,  0.0398,  0.0240,\n",
       "         0.3940,  1.8558,  0.0504,  0.7366, -2.2436, -2.2673, -0.2939, -0.8592,\n",
       "         1.4809])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3010, -0.0103,  0.2229,  0.9927],\n",
       "        [ 1.4477, -0.4305,  0.3110, -0.3709],\n",
       "        [ 0.4433, -2.0150,  0.9629, -0.3408]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod = torch.randn(3, 4)  # Example input; replace with your actual tensor\n",
    "dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming dot_prod is your input tensor with shape [128, 49]\n",
    "\n",
    "\n",
    "# Create an instance of SoftSort\n",
    "soft_sort = SoftSort(tau=1.0, hard=True)\n",
    "\n",
    "# Use SoftSort to rearrange the values based on dot_prod\n",
    "rearranged_values = soft_sort(-1 * dot_prod)\n",
    "\n",
    "# The rearranged_values will have the same shape as dot_prod\n",
    "print(rearranged_values.shape)  # Should be [128, 49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearranged_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3010, -0.0103,  0.2229,  0.9927],\n",
       "        [ 1.4477, -0.4305,  0.3110, -0.3709],\n",
       "        [ 0.4433, -2.0150,  0.9629, -0.3408]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4477, -0.3709, -0.4305,  0.3110])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3709,  1.4477,  0.3110, -0.4305])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = torch.randn(1,1,3)\n",
    "x = torch.randn(1,3,3)\n",
    "dot_prod = torch.matmul(x, keys.transpose(1,2)).squeeze(0) # [1,3]\n",
    "print('dot_prod = ', dot_prod)\n",
    "rearrange = torch.einsum('blk,bk->bl', soft_sort(-dot_prod), dot_prod)\n",
    "print('rearrange = ', rearrange)\n",
    "\n",
    "x_reordered = torch.gather(x, 1, rearrange.unsqueeze(-1).expand(-1, -1, 3).long())  # [B, N, C]\n",
    "print('x_reordered = ', x_reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod =  tensor([[1.7146],\n",
      "        [4.3728],\n",
      "        [1.4453]])\n",
      "rearrange =  tensor([[1.7146],\n",
      "        [4.3728],\n",
      "        [1.4453]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate random tensors\n",
    "keys = torch.randn(1, 1, 3)\n",
    "x = torch.randn(1, 3, 3)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_prod = torch.matmul(x, keys.transpose(1, 2)).squeeze(0)  # [1, 3]\n",
    "print('dot_prod = ', dot_prod)\n",
    "\n",
    "# Calculate rearrange using soft_sort\n",
    "rearranged_values = soft_sort(-dot_prod)  # Assuming soft_sort is defined elsewhere\n",
    "rearrange = torch.einsum('blk,bk->bl', rearranged_values, dot_prod)  # Adjusted based on the original operation\n",
    "print('rearrange = ', rearrange)\n",
    "\n",
    "# Ensure indices are valid for gathering\n",
    "# Clamp to non-negative and check maximum indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (3) for operand 1 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblk,bk->bl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrearranged_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted based on the original operation\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/envs/mambav/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (3) for operand 1 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "torch.einsum('blk,bk->bl', rearranged_values, x)  # Adjusted based on the original operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [4],\n",
      "        [1]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some indices in rearrange are out of bounds for x.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure the maximum index does not exceed bounds of x\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rearrange_indices\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome indices in rearrange are out of bounds for x.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Gather the input x based on the rearrangement\u001b[39;00m\n\u001b[1;32m      9\u001b[0m x_reordered \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(x, \u001b[38;5;241m1\u001b[39m, rearrange_indices\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# [B, N, C]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Some indices in rearrange are out of bounds for x."
     ]
    }
   ],
   "source": [
    "rearrange_indices = rearrange.long()\n",
    "rearrange_indices = rearrange_indices.clamp(min=0)  # Ensure non-negative indices\n",
    "print(rearrange_indices)\n",
    "# Ensure the maximum index does not exceed bounds of x\n",
    "if rearrange_indices.max() >= x.size(1):\n",
    "    raise ValueError(\"Some indices in rearrange are out of bounds for x.\")\n",
    "\n",
    "# Gather the input x based on the rearrangement\n",
    "x_reordered = torch.gather(x, 1, rearrange_indices.unsqueeze(-1).expand(-1, -1, 3))  # [B, N, C]\n",
    "print('x_reordered = ', x_reordered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "class SoftSort(torch.nn.Module):\n",
    "    def __init__(self, tau=1.0, hard=False, pow=1.0):\n",
    "        super(SoftSort, self).__init__()\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "        self.pow = pow\n",
    "\n",
    "    def forward(self, scores: Tensor):\n",
    "        \"\"\"\n",
    "        scores: elements to be sorted. Typical shape: batch_size x n\n",
    "        \"\"\"\n",
    "        scores = scores.unsqueeze(-1)\n",
    "        sorted = scores.sort(descending=True, dim=1)[0]\n",
    "        pairwise_diff = (scores.transpose(1, 2) - sorted).abs().pow(self.pow).neg() / self.tau\n",
    "        P_hat = pairwise_diff.softmax(-1)\n",
    "\n",
    "        if self.hard:\n",
    "            P = torch.zeros_like(P_hat, device=P_hat.device)\n",
    "            P.scatter_(-1, P_hat.topk(1, -1)[1], value=1)\n",
    "            P_hat = (P - P_hat).detach() + P_hat\n",
    "        return P_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "keys = torch.randn(2,1,4)\n",
    "x = torch.randn(2, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1482,  0.2828, -0.4154, -1.1326]],\n",
       "\n",
       "        [[ 1.2830,  2.1231, -0.2014, -0.9114]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4553, -0.6099, -0.6173,  1.0400],\n",
       "         [ 1.4106,  0.6913, -0.3748, -0.3752],\n",
       "         [-0.5062, -1.0152,  1.5643,  0.2343],\n",
       "         [-0.9921,  1.2435,  1.6380,  1.2160],\n",
       "         [-0.5039, -0.7539, -1.5916,  1.4486]],\n",
       "\n",
       "        [[ 1.3329,  0.1666,  2.4799, -2.7505],\n",
       "         [ 0.3777,  1.2816,  0.6190,  0.1405],\n",
       "         [ 0.5517, -0.2661, -0.7200, -1.8182],\n",
       "         [ 0.0213,  0.1991,  1.0101,  0.8899],\n",
       "         [-0.2186, -1.7436,  1.1259,  2.1794]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6165, -0.8435, -0.6210, -0.5669, -0.6141],\n",
       "        [ 4.0713,  2.9528,  1.9451, -0.5644, -6.1952]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SoftSort(hard=True)\n",
    "dot_prod = torch.matmul(x, keys.transpose(1,2)).squeeze(2)\n",
    "dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6165,  0.8435,  0.6210,  0.5669,  0.6141],\n",
       "        [-4.0713, -2.9528, -1.9451,  0.5644,  6.1952]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = ss(-dot_prod)\n",
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6165, -0.8435, -0.6210, -0.6141, -0.5669],\n",
       "        [-6.1952, -0.5644,  1.9451,  2.9528,  4.0713]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk,bk->bl', perm, dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4553, -0.6099, -0.6173,  1.0400],\n",
      "         [ 1.4106,  0.6913, -0.3748, -0.3752],\n",
      "         [-0.5062, -1.0152,  1.5643,  0.2343],\n",
      "         [-0.9921,  1.2435,  1.6380,  1.2160],\n",
      "         [-0.5039, -0.7539, -1.5916,  1.4486]],\n",
      "\n",
      "        [[ 1.3329,  0.1666,  2.4799, -2.7505],\n",
      "         [ 0.3777,  1.2816,  0.6190,  0.1405],\n",
      "         [ 0.5517, -0.2661, -0.7200, -1.8182],\n",
      "         [ 0.0213,  0.1991,  1.0101,  0.8899],\n",
      "         [-0.2186, -1.7436,  1.1259,  2.1794]]])\n",
      "tensor([[[-1.1482,  0.2828, -0.4154, -1.1326]],\n",
      "\n",
      "        [[ 1.2830,  2.1231, -0.2014, -0.9114]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(perm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4553, -0.6099, -0.6173,  1.0400],\n",
       "         [ 1.4106,  0.6913, -0.3748, -0.3752],\n",
       "         [-0.5062, -1.0152,  1.5643,  0.2343],\n",
       "         [-0.5039, -0.7539, -1.5916,  1.4486],\n",
       "         [-0.9921,  1.2435,  1.6380,  1.2160]],\n",
       "\n",
       "        [[-0.2186, -1.7436,  1.1259,  2.1794],\n",
       "         [ 0.0213,  0.1991,  1.0101,  0.8899],\n",
       "         [ 0.5517, -0.2661, -0.7200, -1.8182],\n",
       "         [ 0.3777,  1.2816,  0.6190,  0.1405],\n",
       "         [ 1.3329,  0.1666,  2.4799, -2.7505]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk,bkd->bld', perm, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript l has size 4 for operand 1 which does not broadcast with previously seen size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbkl,bdl->bdk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/envs/mambav/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): subscript l has size 4 for operand 1 which does not broadcast with previously seen size 5"
     ]
    }
   ],
   "source": [
    "torch.einsum('bkl,bdl->bdk', perm, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[[1., 2., 3., 4.],\n",
    "                   [3., 4., 5., 7.]]])\n",
    "importance = torch.Tensor([[2,1,3,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss(-importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 2., 1., 3.],\n",
       "         [7., 4., 3., 5.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.einsum('bkl,bl->bk', ss(-importance), importance))\n",
    "torch.einsum('bkl,bdl->bdk', ss(-importance), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 3 reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import torch.nn as nn\n",
    "from timm.models.registry import register_model\n",
    "import math\n",
    "from timm.models.layers import trunc_normal_, DropPath, LayerNorm2d\n",
    "from timm.models._builder import resolve_pretrained_cfg\n",
    "try:\n",
    "    from timm.models._builder import _update_default_kwargs as update_args\n",
    "except:\n",
    "    from timm.models._builder import _update_default_model_kwargs as update_args\n",
    "from timm.models.vision_transformer import Mlp, PatchEmbed\n",
    "# import torchvision.transforms.functional as T\n",
    "from torchvision import transforms as T\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
    "# from .registry import register_pip_model\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from torch import Tensor\n",
    "\n",
    "class SoftSort(torch.nn.Module):\n",
    "    def __init__(self, tau=1.0, hard=False, pow=1.0):\n",
    "        super(SoftSort, self).__init__()\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "        self.pow = pow\n",
    "\n",
    "    def forward(self, scores: Tensor):\n",
    "        \"\"\"\n",
    "        scores: elements to be sorted. Typical shape: batch_size x n\n",
    "        \"\"\"\n",
    "        scores = scores.unsqueeze(-1)\n",
    "        sorted = scores.sort(descending=True, dim=1)[0]\n",
    "        pairwise_diff = (scores.transpose(1, 2) - sorted).abs().pow(self.pow).neg() / self.tau\n",
    "        P_hat = pairwise_diff.softmax(-1)\n",
    "\n",
    "        if self.hard:\n",
    "            P = torch.zeros_like(P_hat, device=P_hat.device)\n",
    "            P.scatter_(-1, P_hat.topk(1, -1)[1], value=1)\n",
    "            P_hat = (P - P_hat).detach() + P_hat\n",
    "        return P_hat\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {'url': url,\n",
    "            'num_classes': 1000,\n",
    "            'input_size': (3, 224, 224),\n",
    "            'pool_size': None,\n",
    "            'crop_pct': 0.875,\n",
    "            'interpolation': 'bicubic',\n",
    "            'fixed_input_size': True,\n",
    "            'mean': (0.485, 0.456, 0.406),\n",
    "            'std': (0.229, 0.224, 0.225),\n",
    "            **kwargs\n",
    "            }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    'mamba_vision_T': _cfg(url='https://huggingface.co/nvidia/MambaVision-T-1K/resolve/main/mambavision_tiny_1k.pth.tar',\n",
    "                           crop_pct=1.0,\n",
    "                           input_size=(3, 224, 224),\n",
    "                           crop_mode='center'),\n",
    "    'mamba_vision_T2': _cfg(url='https://huggingface.co/nvidia/MambaVision-T2-1K/resolve/main/mambavision_tiny2_1k.pth.tar',\n",
    "                            crop_pct=0.98,\n",
    "                            input_size=(3, 224, 224),\n",
    "                            crop_mode='center'),\n",
    "    'mamba_vision_S': _cfg(url='https://huggingface.co/nvidia/MambaVision-S-1K/resolve/main/mambavision_small_1k.pth.tar',\n",
    "                           crop_pct=0.93,\n",
    "                           input_size=(3, 224, 224),\n",
    "                           crop_mode='center'),\n",
    "    'mamba_vision_B': _cfg(url='https://huggingface.co/nvidia/MambaVision-B-1K/resolve/main/mambavision_base_1k.pth.tar',\n",
    "                           crop_pct=1.0,\n",
    "                           input_size=(3, 224, 224),\n",
    "                           crop_mode='center'),\n",
    "    'mamba_vision_L': _cfg(url='https://huggingface.co/nvidia/MambaVision-L-1K/resolve/main/mambavision_large_1k.pth.tar',\n",
    "                           crop_pct=1.0,\n",
    "                           input_size=(3, 224, 224),\n",
    "                           crop_mode='center'),\n",
    "    'mamba_vision_L2': _cfg(url='https://huggingface.co/nvidia/MambaVision-L2-1K/resolve/main/mambavision_large2_1k.pth.tar',\n",
    "                            crop_pct=1.0,\n",
    "                            input_size=(3, 224, 224),\n",
    "                            crop_mode='center')                                \n",
    "}\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, C, H, W)\n",
    "        window_size: window size\n",
    "        h_w: Height of window\n",
    "        w_w: Width of window\n",
    "    Returns:\n",
    "        local window features (num_windows*B, window_size*window_size, C)\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
    "    windows = x.permute(0, 2, 4, 3, 5, 1).reshape(-1, window_size*window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: local window features (num_windows*B, window_size, window_size, C)\n",
    "        window_size: Window size\n",
    "        H: Height of image\n",
    "        W: Width of image\n",
    "    Returns:\n",
    "        x: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.reshape(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4).reshape(B,windows.shape[2], H, W)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "    \"\"\"Load state_dict to a module.\n",
    "\n",
    "    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n",
    "    Default value for ``strict`` is set to ``False`` and the message for\n",
    "    param mismatch will be shown even if strict is False.\n",
    "\n",
    "    Args:\n",
    "        module (Module): Module that receives the state_dict.\n",
    "        state_dict (OrderedDict): Weights.\n",
    "        strict (bool): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n",
    "        logger (:obj:`logging.Logger`, optional): Logger to log the error\n",
    "            message. If not specified, print function will be used.\n",
    "    \"\"\"\n",
    "    unexpected_keys = []\n",
    "    all_missing_keys = []\n",
    "    err_msg = []\n",
    "\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "    \n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     all_missing_keys, unexpected_keys,\n",
    "                                     err_msg)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(module)\n",
    "    load = None\n",
    "    missing_keys = [\n",
    "        key for key in all_missing_keys if 'num_batches_tracked' not in key\n",
    "    ]\n",
    "\n",
    "    if unexpected_keys:\n",
    "        err_msg.append('unexpected key in source '\n",
    "                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n",
    "    if missing_keys:\n",
    "        err_msg.append(\n",
    "            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
    "\n",
    "    \n",
    "    if len(err_msg) > 0:\n",
    "        err_msg.insert(\n",
    "            0, 'The model and loaded state dict do not match exactly\\n')\n",
    "        err_msg = '\\n'.join(err_msg)\n",
    "        if strict:\n",
    "            raise RuntimeError(err_msg)\n",
    "        elif logger is not None:\n",
    "            logger.warning(err_msg)\n",
    "        else:\n",
    "            print(err_msg)\n",
    "\n",
    "\n",
    "def _load_checkpoint(model,\n",
    "                    filename,\n",
    "                    map_location='cpu',\n",
    "                    strict=False,\n",
    "                    logger=None):\n",
    "    \"\"\"Load checkpoint from a file or URI.\n",
    "\n",
    "    Args:\n",
    "        model (Module): Module to load checkpoint.\n",
    "        filename (str): Accept local filepath, URL, ``torchvision://xxx``,\n",
    "            ``open-mmlab://xxx``. Please refer to ``docs/model_zoo.md`` for\n",
    "            details.\n",
    "        map_location (str): Same as :func:`torch.load`.\n",
    "        strict (bool): Whether to allow different params for the model and\n",
    "            checkpoint.\n",
    "        logger (:mod:`logging.Logger` or None): The logger for error message.\n",
    "\n",
    "    Returns:\n",
    "        dict or OrderedDict: The loaded checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename, map_location=map_location)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\n",
    "            f'No state_dict found in checkpoint file {filename}')\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "\n",
    "    if sorted(list(state_dict.keys()))[0].startswith('encoder'):\n",
    "        state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}\n",
    "\n",
    "    _load_state_dict(model, state_dict, strict, logger)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    Down-sampling block\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 keep_dim=False,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            norm_layer: normalization layer.\n",
    "            keep_dim: bool argument for maintaining the resolution.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        if keep_dim:\n",
    "            dim_out = dim\n",
    "        else:\n",
    "            dim_out = 2 * dim\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim_out, 3, 2, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch embedding block\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans=3, in_dim=64, dim=96):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: number of input channels.\n",
    "            dim: feature size dimension.\n",
    "        \"\"\"\n",
    "        # in_dim = 1\n",
    "        super().__init__()\n",
    "        self.proj = nn.Identity()\n",
    "        self.conv_down = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, in_dim, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_dim, eps=1e-4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_dim, dim, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(dim, eps=1e-4),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.conv_down(x)\n",
    "        return x\n",
    "\n",
    "# [128, 80, 56, 56]\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim,\n",
    "                 drop_path=0.,\n",
    "                 layer_scale=None,\n",
    "                 kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(dim, eps=1e-5)\n",
    "        self.act1 = nn.GELU(approximate= 'tanh')\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(dim, eps=1e-5)\n",
    "        self.layer_scale = layer_scale\n",
    "        if layer_scale is not None and type(layer_scale) in [int, float]:\n",
    "            self.gamma = nn.Parameter(layer_scale * torch.ones(dim))\n",
    "            self.layer_scale = True\n",
    "        else:\n",
    "            self.layer_scale = False\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x # torch.Size([128, 80, 56, 56])\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        if self.layer_scale:\n",
    "            x = x * self.gamma.view(1, -1, 1, 1)\n",
    "        x = input + self.drop_path(x)\n",
    "        return x # torch.Size([128, 80, 56, 56])\n",
    "\n",
    "\n",
    "class MambaVisionMixer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        use_fast_path=True, \n",
    "        layer_idx=None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "        self.use_fast_path = use_fast_path\n",
    "        self.layer_idx = layer_idx\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner, bias=bias, **factory_kwargs)    \n",
    "        self.x_proj = nn.Linear(\n",
    "            self.d_inner//2, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n",
    "        )\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner//2, bias=True, **factory_kwargs)\n",
    "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner//2, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        self.dt_proj.bias._no_reinit = True\n",
    "        A = repeat(\n",
    "            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=self.d_inner//2,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)\n",
    "        self.A_log = nn.Parameter(A_log)\n",
    "        self.A_log._no_weight_decay = True\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner//2, device=device))\n",
    "        self.D._no_weight_decay = True\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "        self.conv1d_x = nn.Conv1d(\n",
    "            in_channels=self.d_inner//2,\n",
    "            out_channels=self.d_inner//2,\n",
    "            bias=conv_bias//2,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner//2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.conv1d_z = nn.Conv1d(\n",
    "            in_channels=self.d_inner//2,\n",
    "            out_channels=self.d_inner//2,\n",
    "            bias=conv_bias//2,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner//2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        hidden_states: (B, L, D)\n",
    "        Returns: same shape as hidden_states\n",
    "        \"\"\"\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        _, seqlen, _ = hidden_states.shape\n",
    "        xz = self.in_proj(hidden_states)\n",
    "        xz = rearrange(xz, \"b l d -> b d l\")\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        x = F.silu(F.conv1d(input=x, weight=self.conv1d_x.weight, bias=self.conv1d_x.bias, padding='same', groups=self.d_inner//2))\n",
    "        z = F.silu(F.conv1d(input=z, weight=self.conv1d_z.weight, bias=self.conv1d_z.bias, padding='same', groups=self.d_inner//2))\n",
    "        x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))\n",
    "        dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        dt = rearrange(self.dt_proj(dt), \"(b l) d -> b d l\", l=seqlen)\n",
    "        B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "        C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "        y = selective_scan_fn(x, \n",
    "                              dt, \n",
    "                              A, \n",
    "                              B, \n",
    "                              C, \n",
    "                              self.D.float(), \n",
    "                              z=None, \n",
    "                              delta_bias=self.dt_proj.bias.float(), \n",
    "                              delta_softplus=True, \n",
    "                              return_last_state=None)\n",
    "        \n",
    "        y = torch.cat([y, z], dim=1)\n",
    "        y = rearrange(y, \"b d l -> b l d\")\n",
    "        out = self.out_proj(y)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = True\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "             q, k, v,\n",
    "                dropout_p=self.attn_drop.p,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 num_heads, \n",
    "                 counter, \n",
    "                 transformer_blocks, \n",
    "                 mlp_ratio=4., \n",
    "                 qkv_bias=False, \n",
    "                 qk_scale=False, \n",
    "                 drop=0., \n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm, \n",
    "                 Mlp_block=Mlp,\n",
    "                 layer_scale=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        if counter in transformer_blocks:\n",
    "            self.mixer = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        else:\n",
    "            self.mixer = MambaVisionMixer(d_model=dim, \n",
    "                                          d_state=8,  \n",
    "                                          d_conv=3,    \n",
    "                                          expand=1\n",
    "                                          )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        use_layer_scale = layer_scale is not None and type(layer_scale) in [int, float]\n",
    "        self.gamma_1 = nn.Parameter(layer_scale * torch.ones(dim))  if use_layer_scale else 1\n",
    "        self.gamma_2 = nn.Parameter(layer_scale * torch.ones(dim))  if use_layer_scale else 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.gamma_1 * self.mixer(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MambaVisionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    MambaVision layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 conv=False,\n",
    "                 downsample=True,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 layer_scale=None,\n",
    "                 layer_scale_conv=None,\n",
    "                 transformer_blocks=[],\n",
    "                 heatmap=[]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            depth: number of layers in each stage.\n",
    "            window_size: window size in each stage.\n",
    "            conv: bool argument for conv stage flag.\n",
    "            downsample: bool argument for down-sampling.\n",
    "            mlp_ratio: MLP ratio.\n",
    "            num_heads: number of heads in each stage.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: drop path rate.\n",
    "            norm_layer: normalization layer.\n",
    "            layer_scale: layer scaling coefficient.\n",
    "            layer_scale_conv: conv layer scaling coefficient.\n",
    "            transformer_blocks: list of transformer blocks.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.heatmap = heatmap\n",
    "        self.conv = conv\n",
    "        self.transformer_block = False\n",
    "        if conv:\n",
    "            self.blocks = nn.ModuleList([ConvBlock(dim=dim,\n",
    "                                                   drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                                   layer_scale=layer_scale_conv)\n",
    "                                         for i in range(depth)])\n",
    "            self.transformer_block = False\n",
    "        else:\n",
    "            self.blocks = nn.ModuleList([Block(dim=dim,\n",
    "                                               counter=i,\n",
    "                                               transformer_blocks=transformer_blocks,\n",
    "                                               num_heads=num_heads,\n",
    "                                               mlp_ratio=mlp_ratio,\n",
    "                                               qkv_bias=qkv_bias,\n",
    "                                               qk_scale=qk_scale,\n",
    "                                               drop=drop,\n",
    "                                               attn_drop=attn_drop,\n",
    "                                               drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                               layer_scale=layer_scale)\n",
    "                                         for i in range(depth)])\n",
    "            self.transformer_block = True\n",
    "\n",
    "        self.downsample = None if not downsample else Downsample(dim=dim)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape  # torch.Size([128, 80, 56, 56])\n",
    "\n",
    "        if self.transformer_block:\n",
    "            pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "            pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = torch.nn.functional.pad(x, (0, pad_r, 0, pad_b))\n",
    "                _, _, Hp, Wp = x.shape\n",
    "            else:\n",
    "                Hp, Wp = H, W\n",
    "            x = window_partition(x, self.window_size)  # torch.Size([128, 196, 320])\n",
    "\n",
    "        heatmaps = []  # Initialize heatmaps list to store activations after each block\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            heatmaps.append(x)  # Store the output of each block\n",
    "\n",
    "        if self.transformer_block:\n",
    "            x = window_reverse(x, self.window_size, Hp, Wp)\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :, :H, :W].contiguous()\n",
    "\n",
    "        if self.downsample is None:\n",
    "            return x, heatmaps  # Return final output and heatmaps\n",
    "        return self.downsample(x), heatmaps  # Return downsampled output and heatmaps\n",
    "\n",
    "class MambaVisionLayer_reorder(nn.Module):\n",
    "    \"\"\"\n",
    "    MambaVision layer\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 conv=False,\n",
    "                 downsample=True,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 layer_scale=None,\n",
    "                 layer_scale_conv=None,\n",
    "                 transformer_blocks = [],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            depth: number of layers in each stage.\n",
    "            window_size: window size in each stage.\n",
    "            conv: bool argument for conv stage flag.\n",
    "            downsample: bool argument for down-sampling.\n",
    "            mlp_ratio: MLP ratio.\n",
    "            num_heads: number of heads in each stage.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: drop path rate.\n",
    "            norm_layer: normalization layer.\n",
    "            layer_scale: layer scaling coefficient.\n",
    "            layer_scale_conv: conv layer scaling coefficient.\n",
    "            transformer_blocks: list of transformer blocks.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.transformer_block = False\n",
    "        self.heatmap = heatmap\n",
    "        self.learnable_keys = nn.Parameter(torch.randn(1, 1, dim)) # ensure dimension = 320\n",
    "        if conv:\n",
    "            self.blocks = nn.ModuleList([ConvBlock(dim=dim,\n",
    "                                                   drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                                   layer_scale=layer_scale_conv)\n",
    "                                                   for i in range(depth)])\n",
    "            self.transformer_block = False\n",
    "        else:\n",
    "            self.blocks = nn.ModuleList([Block(dim=dim,\n",
    "                                               counter=i, \n",
    "                                               transformer_blocks=transformer_blocks,\n",
    "                                               num_heads=num_heads,\n",
    "                                               mlp_ratio=mlp_ratio,\n",
    "                                               qkv_bias=qkv_bias,\n",
    "                                               qk_scale=qk_scale,\n",
    "                                               drop=drop,\n",
    "                                               attn_drop=attn_drop,\n",
    "                                               drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                               layer_scale=layer_scale)\n",
    "                                               for i in range(depth)])\n",
    "            self.transformer_block = True\n",
    "\n",
    "        self.downsample = None if not downsample else Downsample(dim=dim)\n",
    "        self.do_gt = False\n",
    "        self.window_size = window_size\n",
    "        self.soft_sort = SoftSort(hard=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "\n",
    "        if self.transformer_block:\n",
    "            pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "            pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = torch.nn.functional.pad(x, (0,pad_r,0,pad_b))\n",
    "                _, _, Hp, Wp = x.shape\n",
    "            else:\n",
    "                Hp, Wp = H, W\n",
    "            x = window_partition(x, self.window_size)\n",
    "\n",
    "        heatmaps = []  # Initialize heatmaps list to store activations after each block\n",
    "        # perm_matrix = None\n",
    "        for idx, blk in enumerate(self.blocks):\n",
    "            heatmaps.append(x) # Store the output of each block   \n",
    "            x = blk(x)\n",
    "            if idx == 1:\n",
    "                learn_key = self.learnable_keys.expand(B, -1, -1) # [B, 1, C], x [B, N, C]\n",
    "                dot_prod = torch.matmul(x, learn_key.transpose(1,2)).squeeze(2) # [B, N]\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                perm_matrix = self.soft_sort(-dot_prod) # [B, N, N]\n",
    "                x = torch.einsum('blk, bkn -> bln', perm_matrix, x)    \n",
    "            # if idx == 3 and perm_matrix is not None:\n",
    "            #     # Apply reverse permutation to restore the original order\n",
    "            #     perm_matrix_inv = perm_matrix.transpose(1, 2)  # [B, N, N] inverse of the permutation matrix\n",
    "            #     x = torch.einsum('blk, bkn -> bln', perm_matrix_inv, x)       \n",
    "\n",
    "            \n",
    "        if self.transformer_block:\n",
    "            x = window_reverse(x, self.window_size, Hp, Wp)\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :, :H, :W].contiguous()\n",
    "        if self.downsample is None:\n",
    "            return x, heatmaps\n",
    "        return self.downsample(x), heatmaps\n",
    "\n",
    "\n",
    "class MambaVision(nn.Module):\n",
    "    \"\"\"\n",
    "    MambaVision,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 in_dim,\n",
    "                 depths,\n",
    "                 window_size,\n",
    "                 mlp_ratio,\n",
    "                 num_heads,\n",
    "                 drop_path_rate=0.2,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 layer_scale=None,\n",
    "                 layer_scale_conv=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            depths: number of layers in each stage.\n",
    "            window_size: window size in each stage.\n",
    "            mlp_ratio: MLP ratio.\n",
    "            num_heads: number of heads in each stage.\n",
    "            drop_path_rate: drop path rate.\n",
    "            in_chans: number of input channels.\n",
    "            num_classes: number of classes.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            norm_layer: normalization layer.\n",
    "            layer_scale: layer scaling coefficient.\n",
    "            layer_scale_conv: conv layer scaling coefficient.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        num_features = int(dim * 2 ** (len(depths) - 1))\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_embed = PatchEmbed(in_chans=in_chans, in_dim=in_dim, dim=dim)\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        dpr = [x.item() for x in torch.linspace(0, self.drop_path_rate, sum(depths))]\n",
    "        self.levels = nn.ModuleList()\n",
    "        self.heatmap = []\n",
    "        \n",
    "        for i in range(len(depths)):\n",
    "            conv = True if (i == 0 or i == 1) else False\n",
    "            if i == 2:\n",
    "                level = MambaVisionLayer_reorder(dim=int(dim * 2 ** i),\n",
    "                                        depth=depths[i],\n",
    "                                        num_heads=num_heads[i],\n",
    "                                        window_size=window_size[i],\n",
    "                                        mlp_ratio=mlp_ratio,\n",
    "                                        qkv_bias=qkv_bias,\n",
    "                                        qk_scale=qk_scale,\n",
    "                                        conv=conv,\n",
    "                                        drop=drop_rate,\n",
    "                                        attn_drop=attn_drop_rate,\n",
    "                                        drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                                        downsample=(i < 3),\n",
    "                                        layer_scale=layer_scale,\n",
    "                                        layer_scale_conv=layer_scale_conv,\n",
    "                                        transformer_blocks=list(range(depths[i]//2+1, depths[i])) if depths[i]%2!=0 else list(range(depths[i]//2, depths[i])),\n",
    "                                        )\n",
    "            else:\n",
    "                level = MambaVisionLayer(dim=int(dim * 2 ** i),\n",
    "                                        depth=depths[i],\n",
    "                                        num_heads=num_heads[i],\n",
    "                                        window_size=window_size[i],\n",
    "                                        mlp_ratio=mlp_ratio,\n",
    "                                        qkv_bias=qkv_bias,\n",
    "                                        qk_scale=qk_scale,\n",
    "                                        conv=conv,\n",
    "                                        drop=drop_rate,\n",
    "                                        attn_drop=attn_drop_rate,\n",
    "                                        drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                                        downsample=(i < 3),\n",
    "                                        layer_scale=layer_scale,\n",
    "                                        layer_scale_conv=layer_scale_conv,\n",
    "                                        transformer_blocks=list(range(depths[i]//2+1, depths[i])) if depths[i]%2!=0 else list(range(depths[i]//2, depths[i])),\n",
    "                                        )\n",
    "            # self.heatmap.append(heatmap)\n",
    "            self.levels.append(level)\n",
    "        self.norm = nn.BatchNorm2d(num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = nn.Linear(num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, LayerNorm2d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'rpb'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        all_heatmaps = []\n",
    "        # print('x_shape = ', x.shape)\n",
    "        x = self.patch_embed(x) # torch.Size([128, 3, 224, 224]) -> torch.Size([128, 160, 28, 28])\n",
    "        for level in self.levels:\n",
    "            # print(x)\n",
    "            x, heatmaps = level(x)\n",
    "            all_heatmaps.extend(heatmaps)  # Aggregate heatmaps from each layer\n",
    "        # torch.Size([128, 640, 7, 7])\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x) # torch.Size([128, 640, 1, 1])\n",
    "        x = torch.flatten(x, 1) # torch.Size([128, 640])\n",
    "        return x, all_heatmaps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, headmap = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        return x, headmap\n",
    "\n",
    "    def _load_state_dict(self, \n",
    "                         pretrained, \n",
    "                         strict: bool = False):\n",
    "        _load_checkpoint(self, \n",
    "                         pretrained, \n",
    "                         strict=strict)\n",
    "\n",
    "def run_PCA(X):\n",
    "    pca = PCA(n_components=3)\n",
    "    res = pca.fit_transform(X)\n",
    "    return res\n",
    "\n",
    "# Function to generate a unique file name\n",
    "def generate_filename(directory, base_name, extension):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter == 0:\n",
    "            file_name = f\"{base_name}.{extension}\"\n",
    "        else:\n",
    "            file_name = f\"{base_name}_{counter}.{extension}\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            return file_path\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Model initialization settings (same as your code)\n",
    "    kwargs = {'pretrained_cfg': None, 'pretrained_cfg_overlay': None, \n",
    "              'in_chans': 3, 'num_classes': 1000, 'img_size': (224, 224)}\n",
    "    \n",
    "    # checkpoint_path = \"/home/ubuntu/workspace/mambavision_1/mambavision/model_weights/mambavision_tiny_1k.pth.tar\"\n",
    "    # checkpoint_path = \"/home/ubuntu/workspace/mambavision_1/mambavision/model_weights/checkpoint-4.pth.tar\"\n",
    "    checkpoint_path = \"/home/ubuntu/workspace/mambavision_1/test/weights/checkpoint-0.pth.tar\"\n",
    "    image_path = \"/home/ubuntu/workspace/mambavision_1/test/bear.jpg\"\n",
    "    heatmap = []\n",
    "    # Directory where the file will be saved\n",
    "    directory = './bear'\n",
    "    base_name = 'pca_interpolate_order2'\n",
    "    extension = 'jpg'\n",
    "    # Generate a unique file name\n",
    "    file_path = generate_filename(directory, base_name, extension)\n",
    "    \n",
    "    # Model initialization (same as your code)\n",
    "    model = MambaVision(depths=[1, 3, 8, 4],\n",
    "                        num_heads=[2, 4, 8, 16],\n",
    "                        window_size=[8, 8, 14, 7],\n",
    "                        dim=80,\n",
    "                        in_dim=32,\n",
    "                        mlp_ratio=4,\n",
    "                        resolution=224,\n",
    "                        drop_path_rate=0.2, \n",
    "                        **kwargs)\n",
    "\n",
    "    # Load the checkpoint and move the model to the correct device (same as your code)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Preprocess the input image (same as your code)\n",
    "    image = Image.open(image_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Capture model activations (same as your code)\n",
    "    activations = []\n",
    "    outputs = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        outputs[module] = output.detach()\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, heatmaps = model(input_tensor)\n",
    "\n",
    "    # Reshape heatmaps\n",
    "    heatmap_reshape = []\n",
    "    for heatmap in heatmaps:\n",
    "        # print(heatmap.shape)\n",
    "        # Check if the heatmap has 3 dimensions (batch, height*width, feature_dim)\n",
    "        if len(heatmap.shape) == 4:\n",
    "            batch_size, feature_dim, side_length, side_length = heatmap.shape\n",
    "            # Try reshaping the feature maps into square-like dimensions\n",
    "            heatmap_res = heatmap.view(batch_size, side_length * side_length, feature_dim)\n",
    "            heatmap_reshape.append(heatmap_res)\n",
    "        else:\n",
    "            heatmap_reshape.append(heatmap)\n",
    "\n",
    "    # PCA for each heatmap and resize using T.interpolate\n",
    "    pca_images = []\n",
    "    for i, heatmap in enumerate(heatmap_reshape):\n",
    "        # print(f\"Heatmap {i}: Shape = {heatmap.shape}\")\n",
    "        v = heatmap.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Run PCA along the feature dimension (the last dimension)\n",
    "        img = run_PCA(v.cpu())\n",
    "        # Reshape to square for interpolation\n",
    "        h = w = int(math.sqrt(img.shape[0])) #  Adjust this line\n",
    "        img = img.reshape(h, w, -1)\n",
    "        min = img.min(axis=(0, 1))\n",
    "        max = img.max(axis=(0, 1))\n",
    "        img = (img - min) / (max - min)\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        img = T.Resize((224,224), interpolation=T.InterpolationMode.NEAREST)(img)\n",
    "        pca_images.append(img)\n",
    "\n",
    "    # Convert the initial image to [224, 224] format for consistency\n",
    "    initial_image_resized = image.resize((224, 224))\n",
    "    initial_image_np = np.array(initial_image_resized)\n",
    "\n",
    "    # Calculate the total number of images (1 initial + number of heatmaps)\n",
    "    total_images = len(pca_images) + 1\n",
    "\n",
    "    # Calculate the grid size for subplots\n",
    "    cols = 5\n",
    "    rows = (total_images + cols - 1) // cols  # Ensure enough rows to fit all images\n",
    "\n",
    "    # Plot the initial image and resized PCA-reduced images in a single figure\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(12, 12))  # Adjust depending on the number of heatmaps\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # Show the original image in the first subplot\n",
    "    init = axs[0].imshow(initial_image_np)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Initial Image\")\n",
    "    fig.colorbar(init, ax=axs[0], orientation='vertical')\n",
    "    # Show the PCA heatmaps in the remaining subplots\n",
    "    for i in range(1, total_images):\n",
    "        # pca_images[i-1] = (pca_images[i-1] - pca_images[i-1].min())/(pca_images[i-1].max() - pca_images[i-1].min()).astype(int)\n",
    "        im = axs[i].imshow(pca_images[i - 1], cmap='jet')\n",
    "        axs[i].axis('off')\n",
    "        axs[i].set_title(f\"Heatmap {i-1}\")\n",
    "        # Add a color bar next to each heatmap\n",
    "        \n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for j in range(total_images, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 3 + 4 reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(2,3,4)\n",
    "keys = torch.randn(2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6738,  0.5989,  0.7617, -0.7375],\n",
      "         [-1.1518, -0.8145, -1.1547,  0.2358],\n",
      "         [-1.5809, -0.2918, -0.8664,  0.3996]],\n",
      "\n",
      "        [[ 0.2251, -1.5289, -0.4852,  0.8026],\n",
      "         [ 0.2098, -0.5337,  0.2852, -0.6550],\n",
      "         [-0.5836,  1.1107,  0.0136,  0.0351]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5900],\n",
      "         [-0.1405],\n",
      "         [-0.5588]],\n",
      "\n",
      "        [[-1.9241],\n",
      "         [ 1.8041],\n",
      "         [ 0.2175]]])\n"
     ]
    }
   ],
   "source": [
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3975, -0.3533, -0.4494,  0.4351],\n",
       "         [-0.1619, -0.1145, -0.1623,  0.0331],\n",
       "         [-0.8835, -0.1631, -0.4842,  0.2233]],\n",
       "\n",
       "        [[ 0.4332, -2.9418, -0.9336,  1.5442],\n",
       "         [-0.3784,  0.9628, -0.5146,  1.1817],\n",
       "         [ 0.1269, -0.2416, -0.0030, -0.0076]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_params = x*keys\n",
    "-x_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[[-0.1619, -0.1145, -0.1623,  0.4351],\n",
       "         [-0.3975, -0.1631, -0.4494,  0.2233],\n",
       "         [-0.8835, -0.3533, -0.4842,  0.0331]],\n",
       "\n",
       "        [[ 0.4332,  0.9628, -0.0030,  1.5442],\n",
       "         [ 0.1269, -0.2416, -0.5146,  1.1817],\n",
       "         [-0.3784, -2.9418, -0.9336, -0.0076]]]),\n",
       "indices=tensor([[[1, 1, 1, 0],\n",
       "         [0, 2, 0, 2],\n",
       "         [2, 0, 2, 1]],\n",
       "\n",
       "        [[0, 1, 2, 0],\n",
       "         [2, 2, 1, 1],\n",
       "         [1, 0, 0, 2]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(-x_params, k=3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-1, -2, -3]),\n",
       "indices=tensor([2, 1, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = torch.tensor([3,2,1])\n",
    "torch.topk(-x, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
