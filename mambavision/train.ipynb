{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "class SoftSort(torch.nn.Module):\n",
    "    def __init__(self, tau=1.0, hard=False, pow=1.0):\n",
    "        super(SoftSort, self).__init__()\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "        self.pow = pow\n",
    "\n",
    "    def forward(self, scores: Tensor):\n",
    "        \"\"\"\n",
    "        scores: elements to be sorted. Typical shape: batch_size x n\n",
    "        \"\"\"\n",
    "        scores = scores.unsqueeze(-1)\n",
    "        sorted = scores.sort(descending=True, dim=1)[0]\n",
    "        pairwise_diff = (scores.transpose(1, 2) - sorted).abs().pow(self.pow).neg() / self.tau\n",
    "        P_hat = pairwise_diff.softmax(-1)\n",
    "\n",
    "        if self.hard:\n",
    "            P = torch.zeros_like(P_hat, device=P_hat.device)\n",
    "            P.scatter_(-1, P_hat.topk(1, -1)[1], value=1)\n",
    "            P_hat = (P - P_hat).detach() + P_hat\n",
    "        return P_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "ss = SoftSort(hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([[1.0,2.0,5.0]], dtype=torch.float64)\n",
    "mat = ss(-value)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 5.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', mat, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 49, 49])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming dot_prod is your input tensor with shape [128, 49]\n",
    "dot_prod = torch.randn(128, 49)  # Example input; replace with your actual tensor\n",
    "\n",
    "# Create an instance of SoftSort\n",
    "soft_sort = SoftSort(tau=1.0, hard=True)\n",
    "\n",
    "# Use SoftSort to rearrange the values based on dot_prod\n",
    "rearranged_values = soft_sort(dot_prod)\n",
    "\n",
    "# The rearranged_values will have the same shape as dot_prod\n",
    "print(rearranged_values.shape)  # Sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1124,  0.3074, -1.1164,  0.3205, -1.1716,  2.3287, -0.8495,  1.1453,\n",
       "         0.5923, -1.5569, -0.1205, -0.8293, -0.6210,  0.5117, -0.3681,  0.2268,\n",
       "         1.3512,  0.9039, -1.2890,  0.0369, -0.4759,  1.0174, -0.2304, -1.6933,\n",
       "        -1.0110,  0.5619, -1.6541, -1.0059, -0.8064, -0.7143,  0.1221,  0.5179,\n",
       "         1.4403, -0.7206,  1.5478,  0.7394,  0.2535,  0.7082, -2.4703,  1.3706,\n",
       "        -0.3273, -0.7106, -1.3301, -2.0515, -0.0367, -1.3309,  0.0812,  1.7324,\n",
       "        -1.0255])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6355,  1.7180, -0.5275, -0.8325, -0.7621,  0.5230,  1.1946,  0.2854,\n",
       "        -0.3085, -0.0471, -2.4254, -0.4359, -0.7054, -1.3043, -0.3422, -0.0800,\n",
       "         1.1700, -1.1339, -1.0904, -1.1093,  0.3365,  0.6108, -2.7919, -1.8271,\n",
       "        -0.0098, -0.3422, -0.5166,  1.1289,  1.1492, -1.1392,  2.1992, -1.3139,\n",
       "         2.1645,  0.2463,  1.5276, -2.4445, -1.2331,  0.8298,  0.0398,  0.0240,\n",
       "         0.3940,  1.8558,  0.0504,  0.7366, -2.2436, -2.2673, -0.2939, -0.8592,\n",
       "         1.4809])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3010, -0.0103,  0.2229,  0.9927],\n",
       "        [ 1.4477, -0.4305,  0.3110, -0.3709],\n",
       "        [ 0.4433, -2.0150,  0.9629, -0.3408]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod = torch.randn(3, 4)  # Example input; replace with your actual tensor\n",
    "dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming dot_prod is your input tensor with shape [128, 49]\n",
    "\n",
    "\n",
    "# Create an instance of SoftSort\n",
    "soft_sort = SoftSort(tau=1.0, hard=True)\n",
    "\n",
    "# Use SoftSort to rearrange the values based on dot_prod\n",
    "rearranged_values = soft_sort(-1 * dot_prod)\n",
    "\n",
    "# The rearranged_values will have the same shape as dot_prod\n",
    "print(rearranged_values.shape)  # Should be [128, 49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearranged_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3010, -0.0103,  0.2229,  0.9927],\n",
       "        [ 1.4477, -0.4305,  0.3110, -0.3709],\n",
       "        [ 0.4433, -2.0150,  0.9629, -0.3408]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4477, -0.3709, -0.4305,  0.3110])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3709,  1.4477,  0.3110, -0.4305])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('blk, bl -> bk', rearranged_values, dot_prod)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = torch.randn(1,1,3)\n",
    "x = torch.randn(1,3,3)\n",
    "dot_prod = torch.matmul(x, keys.transpose(1,2)).squeeze(0) # [1,3]\n",
    "print('dot_prod = ', dot_prod)\n",
    "rearrange = torch.einsum('blk,bk->bl', soft_sort(-dot_prod), dot_prod)\n",
    "print('rearrange = ', rearrange)\n",
    "\n",
    "x_reordered = torch.gather(x, 1, rearrange.unsqueeze(-1).expand(-1, -1, 3).long())  # [B, N, C]\n",
    "print('x_reordered = ', x_reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod =  tensor([[1.7146],\n",
      "        [4.3728],\n",
      "        [1.4453]])\n",
      "rearrange =  tensor([[1.7146],\n",
      "        [4.3728],\n",
      "        [1.4453]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate random tensors\n",
    "keys = torch.randn(1, 1, 3)\n",
    "x = torch.randn(1, 3, 3)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_prod = torch.matmul(x, keys.transpose(1, 2)).squeeze(0)  # [1, 3]\n",
    "print('dot_prod = ', dot_prod)\n",
    "\n",
    "# Calculate rearrange using soft_sort\n",
    "rearranged_values = soft_sort(-dot_prod)  # Assuming soft_sort is defined elsewhere\n",
    "rearrange = torch.einsum('blk,bk->bl', rearranged_values, dot_prod)  # Adjusted based on the original operation\n",
    "print('rearrange = ', rearrange)\n",
    "\n",
    "# Ensure indices are valid for gathering\n",
    "# Clamp to non-negative and check maximum indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (3) for operand 1 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblk,bk->bl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrearranged_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted based on the original operation\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/envs/mambav/lib/python3.10/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (3) for operand 1 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "torch.einsum('blk,bk->bl', rearranged_values, x)  # Adjusted based on the original operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [4],\n",
      "        [1]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some indices in rearrange are out of bounds for x.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure the maximum index does not exceed bounds of x\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rearrange_indices\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome indices in rearrange are out of bounds for x.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Gather the input x based on the rearrangement\u001b[39;00m\n\u001b[1;32m      9\u001b[0m x_reordered \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(x, \u001b[38;5;241m1\u001b[39m, rearrange_indices\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# [B, N, C]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Some indices in rearrange are out of bounds for x."
     ]
    }
   ],
   "source": [
    "rearrange_indices = rearrange.long()\n",
    "rearrange_indices = rearrange_indices.clamp(min=0)  # Ensure non-negative indices\n",
    "print(rearrange_indices)\n",
    "# Ensure the maximum index does not exceed bounds of x\n",
    "if rearrange_indices.max() >= x.size(1):\n",
    "    raise ValueError(\"Some indices in rearrange are out of bounds for x.\")\n",
    "\n",
    "# Gather the input x based on the rearrangement\n",
    "x_reordered = torch.gather(x, 1, rearrange_indices.unsqueeze(-1).expand(-1, -1, 3))  # [B, N, C]\n",
    "print('x_reordered = ', x_reordered)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
